{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper Functions to keep notebook clean\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define file and folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"DATA\\\\feat\\\\\"\n",
    "\n",
    "imported_data, descriptors = func.load_feat(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Composition_based_feature_vectors', 'Ewald_Site_Energy', 'Smooth_Overlap_of_Atomic_Positions', 'Structural_Heterogenity']\n"
     ]
    }
   ],
   "source": [
    "print(descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data & Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_normalize(data, y_ind):\n",
    "    scaler = StandardScaler()\n",
    "    split_list = []\n",
    "    for df in data:\n",
    "        # Set first row as index\n",
    "        df = df.set_index(df.iloc[:,0])\n",
    "        # Split data and select y_ind column as target\n",
    "        y = df.iloc[:,y_ind]\n",
    "        X = df.drop(df.iloc[:,0:4],axis=1)\n",
    "\n",
    "        # Split into training and test\n",
    "        X_tr_un, X_te_un, y_tr, y_te = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        X_tr = scaler.fit_transform(X_tr_un)\n",
    "        X_te = scaler.transform(X_te_un)\n",
    "        X_tr = normalize(X_tr)\n",
    "        X_te = normalize(X_te)\n",
    "        yXs_tt = [y_tr, y_te, X_tr, X_te]\n",
    "        split_list.append(yXs_tt)\n",
    "    return split_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `y_ind` parameter can be chosen to be either of the 3 NTE classifiers (1: directional NTE, 2: large directional NTE, 3: volumetric NTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ind = 3\n",
    "\n",
    "list_split = split_normalize(imported_data, y_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-/Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample(data, desc):\n",
    "    list_sampled = []\n",
    "    i = 0\n",
    "    for list in data:\n",
    "        print(\"--- Sampling for: \" + str(desc[i]) + \" ---\")\n",
    "        y = list[0]\n",
    "        X = list[2]\n",
    "\n",
    "        # summarize class distribution\n",
    "        print(Counter(y))\n",
    "        # define oversampling strategy\n",
    "        over = RandomOverSampler(sampling_strategy=0.5)\n",
    "        # fit and apply the transform\n",
    "        X, y = over.fit_resample(X, y)\n",
    "        # summarize class distribution\n",
    "        print(Counter(y))\n",
    "        # define undersampling strategy\n",
    "        under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "        # fit and apply the transform\n",
    "        X, y = under.fit_resample(X, y)\n",
    "        # summarize class distribution\n",
    "        print(Counter(y))\n",
    "        list_sampled.append([y, list[1], X, list[3]])\n",
    "        i += 1\n",
    "    return list_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sampling for: Composition_based_feature_vectors ---\n",
      "Counter({0.0: 1812, 1.0: 129})\n",
      "Counter({0.0: 1812, 1.0: 906})\n",
      "Counter({0.0: 1132, 1.0: 906})\n",
      "--- Sampling for: Ewald_Site_Energy ---\n",
      "Counter({0.0: 955, 1.0: 97})\n",
      "Counter({0.0: 955, 1.0: 477})\n",
      "Counter({0.0: 596, 1.0: 477})\n",
      "--- Sampling for: Smooth_Overlap_of_Atomic_Positions ---\n",
      "Counter({0.0: 1112, 1.0: 110})\n",
      "Counter({0.0: 1112, 1.0: 556})\n",
      "Counter({0.0: 695, 1.0: 556})\n",
      "--- Sampling for: Structural_Heterogenity ---\n",
      "Counter({0.0: 1091, 1.0: 103})\n",
      "Counter({0.0: 1091, 1.0: 545})\n",
      "Counter({0.0: 681, 1.0: 545})\n"
     ]
    }
   ],
   "source": [
    "list_sampled = sample(split_list, descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.shape(list_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = 'DATA/pickle/data_final.pkl'\n",
    "data_descriptors = 'DATA/pickle/descriptors.pkl'\n",
    "\n",
    "with open(data_final, 'wb') as f:\n",
    "    pickle.dump(list_sampled, f)\n",
    "\n",
    "with open(data_descriptors, 'wb') as g:\n",
    "    pickle.dump(descriptors, g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9390045621feb823ba4af5b6ebaa3f354f47709dd0151da338676d8e1b29f7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
